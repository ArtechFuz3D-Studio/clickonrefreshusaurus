# Measuring Efficiency with Big O Notation - Time Complexity Equations

### A 'Time Complexity Equation' works by inserting the size of the data-set as an integer 'n', and returning the number of operations that need to be conducted by the computer before the function can finish. 

* N = The size of the Data-Set
    * The amount of elements contained within the Data Structure

###### As an example, if we have an array with the size of 10.
    N = 10
    Efficiency Equations:
        Accessing = 10
        Searching = 10
        Inserting = 10
        Deleting = 10
    << returned back is the number of operations needed to be computed before the function completes.
    
### Note 
We always use the worst case scenario when judging these Data Structures. We want to prepare for the worst and know which data structures are going to be able to perform under the worst conditions.

# Why 'Big O Notation?'
The syntax for the Time Complexity Equations includes a capitalized O and then a set of parenthesis:
## O()
## The parenthesis houses the functions which will correctly return the number of operations needed to be run by the computer.

Example:
```
Function =randomFunction
    randomFunction()
timeComplexityEquation = O()
    O(2)
```
#### We pronounce this time complexity equation "O of 2", meaning it takes 2 operations from the computer before the randomFunction can finish.
If the 'tCE' was O with (5), we would say 'O of 5', and so on.
These are examples of tCE that run in constant time, meaning no matter the size of the data-set, they usually take the same number of instructions to run. But that is usually not going to be the case when it comes to the 4 criteria we want to measure for our data structures.

Most of the time, the integer 'n' (the amount of elements in the data set) is going to have some effect on how many oiperations it takes to do things. A larger data set means taking more instructions to run through the data set.

#### We measure the efficiency of these 4 functionalities and the '# of operations performed' because measuring how long the function takes to run would be silly.
    Measuring by time is biased to hardware

## Summary:
### We measure EFFICIENCY based on 4 metrics:
    Accessing
    Searching
    Inserting
    Deleting

Each of these criteria is modeled by an equation, which takes on the size of the data set in 'number of elements(n)' and returns the number of operations ( not how long) needed to be performed by the computer to complete.
By measuring these 4 things we can get a good idea at what the Data Structure is good at, and what it is bad at.

### This isnt the be all end, but is incredibly useful
Some have specific quarks or features which seperate them from the rest.

# What are the 5 most common time complexity efficiency equations?

## The Gold Standard O(1)
The best that a Data Structure can score on each criteria is o(1), no matter what the size of the data set, this score equates to the operation being performed in a single instruction. It is instantaneous.

<iframe width="730" height="515" src="https://www.youtube.com/embed/SBT1xtuEAzA?start=397" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Example of tCE of O(1):

| Size of Data Set | Operations Required |
| -------- | -------- |
| 1        | 1        |
| 100      | 1        |
| 100,000 | 1        |

When we look at a graph of 'Volume of Data' vs '# of Operations', the line remains constant at 1, no matter the volume of data being used, the computer can complete that task in a single instrucction, wether, accessing, reading, or modifying data.

O(1) is the Gold Standard, top of its class efficiency equation.

## The next fastest is 'O(log n)'
While not as fast as instantaneous, it is still very fast.
Note that while it is not as fast as O(1), it is still faster than the next best time complexity equation.
[Refer to the images in the video for a visual representation.](https://youtu.be/SBT1xtuEAzA?t=430)
<iframe width="730" height="515" src="https://www.youtube.com/embed/SBT1xtuEAzA?start=430" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The defining charateristic of logarithmic algorithm is that it gets more efficient as the size of the dat set increases.
Initially, increasing number of operations sky rockets as the size of data set grows but it eventually hits a plateau.

A good ex of a non data structures related function, which has a tC of O(log n), is the binary search. [Delve deeper into binary search](https://www.youtube.com/watch?v=tVftenlYw44&t=5567).

## O(n) or O of n, is the next biggest tCE
The graph of Volume of Data vs # of Instructions for this function, is linear, meaning that as the size of the data set increases. so do the number of operations.

View the graph:

<iframe width="730" height="515" src="https://www.youtube.com/embed/SBT1xtuEAzA?start=517" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


| Size of Data Set | Operations Required |
| -------- | -------- |
| 10       | 10        |
| 100      | 100        |
| 100,000 | 100,000       |

This is really the last good efficiency equation that is exists. Anything above this is considered inefficient and not very practical.

## The first relatively bad efficiency equation is O(n log n)
The graph of Volume of Data vs # of Instructions for this function increases almost exponentially but then increases linearly at larger volumes, unlike O(log n) it doesnt get better at efficiency as time or data volume increases.

<iframe width="730" height="515" src="https://www.youtube.com/embed/SBT1xtuEAzA?start=565" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## The last 2 equations are O(n2) and O(2n). O of n squared and O of 2 to the N
These are both incredibly inefficient and should be avoided.
They both increase exponentially, the larger te data set becomes, the more inefficient it gets.

<iframe width="700" height="515" src="https://www.youtube.com/embed/SBT1xtuEAzA?start=589" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

While there are other tCE that exist, such as O of n factorial which is even worse, the data structures we will learn about here will not have tCE that exist outside of the above mentioned 5.

### Time Complexity Equations are not the only metric that you should rely on to guage which data structure to use.
You will notice that some data structures dont seem very efficient based on the tCE but provide some other functionality that make them extremely useful for programmers.


[Back](2. Big O Notation.md)
[Next](4. )
[Home](0. Introduction)